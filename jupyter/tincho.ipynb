{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from marca import MarcaSpider\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#DB NER\n",
    "dbdb = os.getenv('MONGO_DATABASE',None)\n",
    "dbhost = os.getenv('MONGO_HOST',None)\n",
    "dbport = os.getenv('MONGO_PORT',None)\n",
    "dbuser = os.getenv('MONGO_USER',None)\n",
    "dbpass = os.getenv('MONGO_PASS',None)\n",
    "dbcoll = os.getenv('MONGO_COLL',None)\n",
    "\n",
    "\n",
    "#Info corridas\n",
    "otherClient = MongoClient('mongodb://'+dbuser+':'+dbpass+'@'+dbhost+':'+dbport+'/'+dbdb)\n",
    "db = otherClient[dbdb]\n",
    "news_data = db['news-data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print len(list(news_data.find({\"team\": 81})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapy.utils.log:Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Scrapy 1.0.3 started (bot: scrapybot)\n",
      "INFO:scrapy.utils.log:Optional features available: ssl, http11, boto\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Optional features available: ssl, http11, boto\n",
      "INFO:scrapy.utils.log:Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Overridden settings: {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
      "INFO:scrapy.middleware:Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "2016-10-20 01:28:28 [scrapy] INFO: Enabled extensions: CloseSpider, TelnetConsole, LogStats, CoreStats, SpiderState\n",
      "DEBUG:boto:Retrieving credentials from metadata server.\n",
      "2016-10-20 01:28:28 [boto] DEBUG: Retrieving credentials from metadata server.\n",
      "2016-10-20 01:28:28 [boto] DEBUG: Retrieving credentials from metadata server.\n",
      "2016-10-20 01:28:28 [boto] DEBUG: Retrieving credentials from metadata server.\n",
      "2016-10-20 01:28:28 [boto] DEBUG: Retrieving credentials from metadata server.\n",
      "2016-10-20 01:28:28 [boto] DEBUG: Retrieving credentials from metadata server.\n",
      "ERROR:boto:Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "2016-10-20 01:28:29 [boto] ERROR: Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "2016-10-20 01:28:29 [boto] ERROR: Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "2016-10-20 01:28:29 [boto] ERROR: Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "2016-10-20 01:28:29 [boto] ERROR: Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "2016-10-20 01:28:29 [boto] ERROR: Caught exception reading instance data\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ds/local/lib/python2.7/site-packages/boto/utils.py\", line 210, in retry_url\n",
      "    r = opener.open(req, timeout=timeout)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 404, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 422, in _open\n",
      "    '_open', req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 382, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1214, in http_open\n",
      "    return self.do_open(httplib.HTTPConnection, req)\n",
      "  File \"/usr/lib/python2.7/urllib2.py\", line 1184, in do_open\n",
      "    raise URLError(err)\n",
      "URLError: <urlopen error timed out>\n",
      "ERROR:boto:Unable to read instance data, giving up\n",
      "2016-10-20 01:28:29 [boto] ERROR: Unable to read instance data, giving up\n",
      "2016-10-20 01:28:29 [boto] ERROR: Unable to read instance data, giving up\n",
      "2016-10-20 01:28:29 [boto] ERROR: Unable to read instance data, giving up\n",
      "2016-10-20 01:28:29 [boto] ERROR: Unable to read instance data, giving up\n",
      "2016-10-20 01:28:29 [boto] ERROR: Unable to read instance data, giving up\n",
      "INFO:scrapy.middleware:Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats\n",
      "INFO:scrapy.middleware:Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware\n",
      "INFO:scrapy.middleware:Enabled item pipelines: \n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled item pipelines: \n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled item pipelines: \n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled item pipelines: \n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled item pipelines: \n",
      "2016-10-20 01:28:29 [scrapy] INFO: Enabled item pipelines: \n",
      "INFO:scrapy.core.engine:Spider opened\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Spider opened\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Spider opened\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Spider opened\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Spider opened\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Spider opened\n",
      "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2016-10-20 01:28:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "DEBUG:scrapy.telnet:Telnet console listening on 127.0.0.1:6026\n",
      "2016-10-20 01:28:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026\n",
      "2016-10-20 01:28:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026\n",
      "2016-10-20 01:28:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026\n",
      "2016-10-20 01:28:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026\n",
      "2016-10-20 01:28:29 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6026\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-36906eabf0df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMarcaSpider\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# the script will block here until the crawling is finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/scrapy/crawler.pyc\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, stop_after_crawl)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mtp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjustPoolsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'REACTOR_THREADPOOL_MAXSIZE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddSystemEventTrigger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'before'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shutdown'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocking call\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dns_resolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \"\"\"\n\u001b[0;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/ds/local/lib/python2.7/site-packages/twisted/internet/base.pyc\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "process.crawl(MarcaSpider)\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DeferredList at 0x7fedd05e0518 current result: []>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
